{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/+AjlxL2B45Ybu0gcvo+v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AyanChattaraj/geb-ai-demo/blob/main/r_2_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-google-genai langchain_mistralai mistralai"
      ],
      "metadata": {
        "id": "FAdscYV0ny7w"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")\n",
        "\n",
        "if \"MISTRAL_API_KEY\" not in os.environ:\n",
        "    os.environ[\"MISTRAL_API_KEY\"] = getpass.getpass(\"Enter your Mistral AI API key: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eykx0w1ywChJ",
        "outputId": "2f052f6c-dd88-492e-facc-1e08c7f1a627"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Google AI API key: ··········\n",
            "Enter your Mistral AI API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms.base import LLM\n",
        "\n",
        "class CodestralLLM(LLM):\n",
        "    def __init__(self, api_key, model_name=\"codestral-latest\"):\n",
        "        from mistralai import Mistral\n",
        "        self.client = Mistral(api_key=api_key)\n",
        "        self.model = model_name\n",
        "\n",
        "    def _call(self, prompt, stop=None):\n",
        "        response = self.client.completions.create(\n",
        "            model=self.model,\n",
        "            prompt=prompt,\n",
        "            stop=stop,\n",
        "            temperature=0,\n",
        "        )\n",
        "        return response.choices[0].text"
      ],
      "metadata": {
        "id": "EUlCjMrGHMLi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from mistralai import Mistral\n",
        "from langchain_mistralai import ChatMistralAI\n",
        "\n",
        "class RToPythonConverter:\n",
        "    def __init__(self, model=\"gemini-2.0-flash-001\", temperature=0):\n",
        "        \"\"\"Initialize the Gemini AI model.\"\"\"\n",
        "        self.llm = ChatGoogleGenerativeAI(\n",
        "            model=model,\n",
        "            temperature=temperature,\n",
        "            max_tokens=None,\n",
        "            timeout=None,\n",
        "            max_retries=2,\n",
        "        )\n",
        "        self.template = \"\"\"You are an expert AI programmer skilled in multiple languages.\n",
        "Your task is to analyze the provided **R** program, understand its logic, functions, and dependencies, then generate an equivalent **Python** program.\n",
        "Follow these rules:\n",
        "1. Identify the core logic of the R program and translate it **accurately** into Python.\n",
        "2. Replace **R-specific functions** with the appropriate Python equivalents.\n",
        "3. Ensure external libraries used in R (e.g., `ggplot2`, `dplyr`) are mapped to suitable Python packages (e.g., `matplotlib`, `pandas`).\n",
        "4. Maintain the readability and efficiency of the translated Python code.\n",
        "5. If an R function has no direct Python equivalent, construct the logic manually.\n",
        "6. 1st para of Python program should briefly summarize the code as docstring\n",
        "R Code:\n",
        "{r_code}\n",
        "\n",
        "Python Code:\n",
        " \"\"\"\n",
        "        self.prompt = PromptTemplate(input_variables=[\"r_code\"], template=self.template)\n",
        "        self.llm_chain = LLMChain(prompt=self.prompt, llm=self.llm)\n",
        "\n",
        "    def convert_r_to_python(self, r_code):\n",
        "        \"\"\"Convert R code to Python using Gemini AI.\"\"\"\n",
        "        response = self.llm_chain.run(r_code)\n",
        "\n",
        "        # Extract Python code within ```python ... ```\n",
        "        match = re.search(r\"```python(.*?)```\", response, re.DOTALL)\n",
        "        clean_response = match.group(1).strip() if match else response.strip()\n",
        "\n",
        "        return clean_response\n",
        "\n",
        "    def save_python_code(self, python_code, output_file):\n",
        "        \"\"\"Save generated Python code to a file.\"\"\"\n",
        "        output_dir = os.path.dirname(output_file)\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        with open(output_file, \"w\") as file:\n",
        "            file.write(python_code)\n",
        "\n",
        "        print(f\"\\n✅ Python code has been saved to: {output_file}\")\n",
        "\n",
        "    def convert_r_file_to_python(self, r_file_path, output_file):\n",
        "        \"\"\"Read an R program from file, convert it to Python, and save the result.\"\"\"\n",
        "        if not os.path.exists(r_file_path):\n",
        "            print(f\"❌ Error: File '{r_file_path}' not found.\")\n",
        "            return\n",
        "\n",
        "        # Read the R program file\n",
        "        with open(r_file_path, \"r\") as file:\n",
        "            r_code = file.read()\n",
        "\n",
        "        # Convert the R code to Python\n",
        "        python_code = self.convert_r_to_python(r_code)\n",
        "\n",
        "        # Save the generated Python code to file\n",
        "        self.save_python_code(python_code, output_file)\n",
        "\n",
        "    def convert_and_save(self, r_file_path, output_file, model_config):\n",
        "      \"\"\"\n",
        "        Converts R code to Python using specified model and saves to a folder.\n",
        "      \"\"\"\n",
        "      model_name = model_config[\"model_name\"]\n",
        "      model_params = model_config.get(\"model_params\", {})  # Get params if present\n",
        "\n",
        "      # Create model instance based on model_name\n",
        "      if model_name == \"gemini-2.0-flash-001\":\n",
        "        self.llm = ChatGoogleGenerativeAI(\n",
        "              model=model_name,\n",
        "              **model_params)\n",
        "      elif model_name == \"codestral-mamba-latest\":\n",
        "        self.llm = ChatMistralAI(\n",
        "              model=model_name,\n",
        "              **model_params\n",
        "                )\n",
        "      else:\n",
        "          raise ValueError(f\"Unsupported model: {model_name}\")\n",
        "\n",
        "      # Update LLMChain with the new model\n",
        "      self.llm_chain = LLMChain(prompt=self.prompt, llm=self.llm)\n",
        "\n",
        "\n",
        "      self.convert_r_file_to_python(r_file_path, output_file)\n",
        "\n",
        "    def convert_and_save_all(self, r_directory, output_dir, model_config):\n",
        "      \"\"\"\n",
        "      Converts all R files in a directory to Python and saves them.\n",
        "      \"\"\"\n",
        "      for filename in os.listdir(r_directory):\n",
        "          if filename.endswith(\".r\"):  # Process only .r files\n",
        "              r_file_path = os.path.join(r_directory, filename)\n",
        "              # Create output file name with .py extension\n",
        "              output_file = os.path.join(output_dir, model_config[\"model_name\"], filename[:-2] + \".py\")\n",
        "              self.convert_and_save(r_file_path, output_file, model_config)  # Use existing method\n",
        "\n",
        "\n",
        "# Test the reusable method with a sample R program file\n",
        "if __name__ == \"__main__\":\n",
        "    r_directory = \"/content/Input\"  # Input directory containing R files\n",
        "    output_base_dir = \"/content/Output\"\n",
        "\n",
        "    model_configs = [\n",
        "        {\"model_name\": \"gemini-2.0-flash-001\",\n",
        "         \"model_params\":\n",
        "          {\n",
        "              \"temperature\": 0,\n",
        "              \"max_tokens\":None,\n",
        "              \"timeout\":None,\n",
        "              \"max_retries\":2,\n",
        "              \"api_key\":os.environ.get(\"GOOGLE_API_KEY\")\n",
        "              }\n",
        "        },\n",
        "\n",
        "        {\"model_name\": \"codestral-mamba-latest\",\n",
        "         \"model_params\":\n",
        "          {\n",
        "              \"temperature\": 0,\n",
        "              \"max_retries\":2,\n",
        "              \"api_key\":os.environ.get(\"MISTRAL_API_KEY\")\n",
        "              }\n",
        "         },\n",
        "    ]\n",
        "\n",
        "    converter = RToPythonConverter()\n",
        "\n",
        "    for config in model_configs:\n",
        "        converter.convert_and_save_all(r_directory, output_base_dir, config)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rvqQMYUxxTa",
        "outputId": "cd28bd6e-c26a-48ad-84be-3996e5dd7fe6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Python code has been saved to: /content/Output/gemini-2.0-flash-001/regression.py\n",
            "\n",
            "✅ Python code has been saved to: /content/Output/gemini-2.0-flash-001/box_plot.py\n",
            "\n",
            "✅ Python code has been saved to: /content/Output/gemini-2.0-flash-001/data_manipulation.py\n",
            "\n",
            "✅ Python code has been saved to: /content/Output/codestral-mamba-latest/regression.py\n",
            "\n",
            "✅ Python code has been saved to: /content/Output/codestral-mamba-latest/box_plot.py\n",
            "\n",
            "✅ Python code has been saved to: /content/Output/codestral-mamba-latest/data_manipulation.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: delete Output dir\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Define the directory to be deleted\n",
        "dir_path = \"/content/Output\"\n",
        "\n",
        "# Check if the directory exists before attempting to delete it\n",
        "if os.path.exists(dir_path):\n",
        "  try:\n",
        "    shutil.rmtree(dir_path)\n",
        "    print(f\"Directory '{dir_path}' and its contents have been successfully deleted.\")\n",
        "  except OSError as e:\n",
        "    print(f\"Error: {e}\")\n",
        "else:\n",
        "  print(f\"Directory '{dir_path}' does not exist.\")\n"
      ],
      "metadata": {
        "id": "xMDgEgpN4QXO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7688a0ae-c659-4455-8d2b-0141c9c9616e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory '/content/Output' and its contents have been successfully deleted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sample output by Mistral**"
      ],
      "metadata": {
        "id": "E_UJX4ljMjyI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VL_kNGEcMrRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sample output by gemini**"
      ],
      "metadata": {
        "id": "xIqqlHGLMvF6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YJV4I3G8My5c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}